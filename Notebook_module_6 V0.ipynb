{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP3lXZ5/rSQoXe7Nbd0s/jm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Z0rr09/OD-WeaponDetection/blob/master/Notebook_module_6%20V0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import Libraries:\n",
        "We'll start by importing the necessary libraries. For this project, we'll need libraries such as PyTorch, torchvision, matplotlib, NumPy, and others."
      ],
      "metadata": {
        "id": "KZ5HvbPuROot"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.models.detection import fasterrcnn_resnet50_fpn\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.datasets import VOCDetection\n",
        "from PIL import Image\n",
        "import xml.etree.ElementTree as ET\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "L3v1dHCuRKE9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Set Device:\n",
        "We'll set the device to GPU if available, otherwise, we'll use the CPU."
      ],
      "metadata": {
        "id": "G7wVMbo2RfhU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "metadata": {
        "id": "0azFHQSmRJ99"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define Dataset Paths: We'll specify the paths to the image and annotation folders containing the dataset."
      ],
      "metadata": {
        "id": "6FHU_r9LRu1Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "image_folder = 'https://github.com/Z0rr09/OD-WeaponDetection/tree/master/Pistol%20detection/Weapons'\n",
        "annotation_folder = 'https://github.com/Z0rr09/OD-WeaponDetection/tree/master/Pistol%20detection/xmls'\n"
      ],
      "metadata": {
        "id": "z3YmULxCRJ3j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Define Transform:\n",
        "We'll define the transformations to apply to the images before feeding them into the model."
      ],
      "metadata": {
        "id": "2cvrEomASAxt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "])"
      ],
      "metadata": {
        "id": "74W4kHRWRJyl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load Dataset: We'll load the dataset using the VOCDetection class from torchvision.datasets, providing the dataset's root directory, year, image set, and the defined transform."
      ],
      "metadata": {
        "id": "ryRilxCOSC-c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = VOCDetection(root=image_folder, year='2007', image_set='trainval', download=False, transform=transform)\n"
      ],
      "metadata": {
        "id": "L3bR0gs2RJtV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define Function to Show Bounding Boxes: We'll write a function to visualize bounding boxes on images."
      ],
      "metadata": {
        "id": "se3sBVXQSTpK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def show_image_with_boxes(image, target):\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    image = image.permute(1, 2, 0)\n",
        "    plt.imshow(image)\n",
        "    for box in target['boxes']:\n",
        "        xmin, ymin, xmax, ymax = box\n",
        "        plt.plot([xmin, xmax, xmax, xmin, xmin], [ymin, ymin, ymax, ymax, ymin], color='r')\n",
        "    plt.axis('off')\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "IE9uzs_5RJoE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Parse XML Annotations: We'll implement a function to parse XML annotation files associated with each image, extracting bounding box coordinates and class labels."
      ],
      "metadata": {
        "id": "mYIfebwoShIy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def parse_annotation(xml_file):\n",
        "    tree = ET.parse(xml_file)\n",
        "    root = tree.getroot()\n",
        "    boxes = []\n",
        "    labels = []\n",
        "    for obj in root.findall('object'):\n",
        "        xmin = int(obj.find('bndbox/xmin').text)\n",
        "        ymin = int(obj.find('bndbox/ymin').text)\n",
        "        xmax = int(obj.find('bndbox/xmax').text)\n",
        "        ymax = int(obj.find('bndbox/ymax').text)\n",
        "        boxes.append([xmin, ymin, xmax, ymax])\n",
        "        labels.append(obj.find('name').text)\n",
        "    return {'boxes': torch.tensor(boxes, dtype=torch.float32), 'labels': labels}\n"
      ],
      "metadata": {
        "id": "fZs_h8I4RJeW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Custom Collate Function: We'll define a custom collate function to preprocess the data and annotations before passing them to the model."
      ],
      "metadata": {
        "id": "QnASQ0v-Smpi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def collate_fn(batch):\n",
        "    images = []\n",
        "    targets = []\n",
        "    for image, target in batch:\n",
        "        images.append(image)\n",
        "        targets.append(target)\n",
        "    return images, targets\n"
      ],
      "metadata": {
        "id": "toPY5cwXRJXv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create DataLoader: We'll create a DataLoader to iterate over the dataset in batches, utilizing the custom collate function."
      ],
      "metadata": {
        "id": "7bWkM3hxSy8q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_loader = DataLoader(dataset, batch_size=4, shuffle=True, collate_fn=collate_fn)\n"
      ],
      "metadata": {
        "id": "pQAbftOCSnFf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load Pre-trained Model: We'll load a pre-trained Faster R-CNN model using the fasterrcnn_resnet50_fpn function from torchvision.models.detection."
      ],
      "metadata": {
        "id": "Eb8KUG5_TLAs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = fasterrcnn_resnet50_fpn(pretrained=True)\n",
        "model.to(device)\n"
      ],
      "metadata": {
        "id": "CbHDaev2TO7N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define Optimizer and Loss Function: We'll define the optimizer and loss function for training the model. For this example, let's use the SGD optimizer and the Smooth L1 loss."
      ],
      "metadata": {
        "id": "1k63zAtSTXSR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.005, momentum=0.9, weight_decay=0.0005)\n",
        "loss_function = torch.nn.SmoothL1Loss()\n"
      ],
      "metadata": {
        "id": "HmOAVntpTO4F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Function to Show Example Photos: We'll create a function to randomly select and display a few example photos from the dataset."
      ],
      "metadata": {
        "id": "GqIMtzmhTcHW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def show_example_photos(dataset, num_photos=3):\n",
        "    indices = torch.randperm(len(dataset))[:num_photos]\n",
        "    for idx in indices:\n",
        "        image, target = dataset[idx]\n",
        "        show_image_with_boxes(image, target)\n"
      ],
      "metadata": {
        "id": "MnVlGuSNTOtS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load Subset of Dataset: We'll load a subset of the dataset for training, specifying the subset size and creating a DataLoader for it."
      ],
      "metadata": {
        "id": "ne5-4mS_Tkp3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "subset_indices = torch.randperm(len(dataset))[:100]  # Load 100 samples as a subset\n",
        "subset_dataset = torch.utils.data.Subset(dataset, subset_indices)\n",
        "subset_data_loader = DataLoader(subset_dataset, batch_size=4, shuffle=True, collate_fn=collate_fn)\n"
      ],
      "metadata": {
        "id": "GQvXrfjsTOqm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training Loop: We'll implement a training loop to train the model on the subset of the dataset, iterating over the DataLoader and optimizing the model parameters."
      ],
      "metadata": {
        "id": "o72XlTvMTr4V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, optimizer, loss_function, data_loader, num_epochs=5):\n",
        "    for epoch in range(num_epochs):\n",
        "        total_loss = 0.0\n",
        "        for images, targets in data_loader:\n",
        "            images = [image.to(device) for image in images]\n",
        "            targets = [{k: v.to(device) for k, v in target.items()} for target in targets]\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss_dict = model(images, targets)\n",
        "            losses = sum(loss for loss in loss_dict.values())\n",
        "\n",
        "            losses.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += losses.item()\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {total_loss/len(data_loader)}\")\n",
        "\n",
        "train(model, optimizer, loss_function, subset_data_loader)\n"
      ],
      "metadata": {
        "id": "dn9qFPrxTOno"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Visualize Sample Prediction: We'll visualize a sample prediction by making inference on a single image from the dataset and displaying the original image with predicted bounding boxes."
      ],
      "metadata": {
        "id": "Iu9yTboBTwPH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def visualize_prediction(model, dataset):\n",
        "    idx = torch.randint(len(dataset), (1,)).item()\n",
        "    image, target = dataset[idx]\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        prediction = model([image.to(device)])\n",
        "    model.train()\n",
        "\n",
        "    image = image.permute(1, 2, 0).cpu().numpy()\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.imshow(image)\n",
        "    for box in prediction[0]['boxes']:\n",
        "        xmin, ymin, xmax, ymax = box.cpu().numpy()\n",
        "        plt.plot([xmin, xmax, xmax, xmin, xmin], [ymin, ymin, ymax, ymax, ymin], color='r')\n",
        "    plt.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "visualize_prediction(model, dataset)\n"
      ],
      "metadata": {
        "id": "LG9fwcQZTOkn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model Evaluation: We'll implement a function to evaluate the model's performance on a separate validation or test dataset. We'll compute relevant metrics such as precision, recall, F1-score, and mean average precision (mAP) for object detection tasks."
      ],
      "metadata": {
        "id": "GIZR4Z3oUL-U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(model, data_loader):\n",
        "    model.eval()\n",
        "    all_predictions = []\n",
        "    all_targets = []\n",
        "    with torch.no_grad():\n",
        "        for images, targets in data_loader:\n",
        "            images = [image.to(device) for image in images]\n",
        "            targets = [{k: v.to(device) for k, v in target.items()} for target in targets]\n",
        "\n",
        "            predictions = model(images)\n",
        "            all_predictions.extend(predictions)\n",
        "            all_targets.extend(targets)\n",
        "\n",
        "    # Compute evaluation metrics (e.g., precision, recall, F1-score, mAP)\n",
        "    # You can use libraries like scikit-learn or torchvision's ObjectDetectionMetric\n",
        "    # to compute these metrics\n",
        "    # Example:\n",
        "    # mAP = compute_mean_average_precision(all_predictions, all_targets)\n",
        "    # precision, recall, f1_score = compute_precision_recall_f1(all_predictions, all_targets)\n",
        "\n",
        "    # Print or log the evaluation results\n",
        "    # print(f\"mAP: {mAP}, Precision: {precision}, Recall: {recall}, F1-score: {f1_score}\")\n"
      ],
      "metadata": {
        "id": "KW1CEZAlUJcC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Save and Load Model: We'll add functionality to save trained models to disk and load them for inference or further training. This allows for easy model deployment and reuse without retraining from scratch."
      ],
      "metadata": {
        "id": "7m0ATdu3UQJM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def save_model(model, filepath):\n",
        "    torch.save(model.state_dict(), filepath)\n",
        "\n",
        "def load_model(model, filepath):\n",
        "    model.load_state_dict(torch.load(filepath))\n",
        "    model.eval()\n"
      ],
      "metadata": {
        "id": "AT1pOyDnUJY7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model Interpretability: We'll explore methods for interpreting and visualizing model predictions, such as class activation maps or occlusion sensitivity. This can provide insights into the model's decision-making process and help diagnose errors."
      ],
      "metadata": {
        "id": "6U-XwksOUT-b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example function for generating class activation maps (CAM)\n",
        "# Requires modification to adapt to the specific model architecture\n",
        "def generate_cam(model, image):\n",
        "    # Forward pass through the model to obtain feature maps\n",
        "    feature_maps = model.backbone(image.unsqueeze(0))\n",
        "\n",
        "    # Perform global average pooling on the feature maps\n",
        "    pooled_features = torch.mean(feature_maps, dim=[2, 3])\n",
        "\n",
        "    # Perform linear transformation to obtain class activation map (CAM)\n",
        "    cam = model.roi_heads.box_predictor.cls_score(pooled_features)\n",
        "\n",
        "    # Normalize CAM\n",
        "    cam = torch.nn.functional.interpolate(cam, size=image.shape[-2:], mode='bilinear', align_corners=False)\n",
        "    cam = torch.nn.functional.softmax(cam, dim=1)\n",
        "\n",
        "    return cam\n"
      ],
      "metadata": {
        "id": "lt0Wao-xUJUU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pipeline Optimization: We'll profile the codebase to identify bottlenecks or areas for optimization, such as data loading, model inference, or gradient computation. We can then optimize these components to improve overall training speed and efficiency."
      ],
      "metadata": {
        "id": "KA1X7tTDUYvQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example: Profile data loading time\n",
        "import time\n",
        "start_time = time.time()\n",
        "# Code for data loading\n",
        "end_time = time.time()\n",
        "print(f\"Data loading time: {end_time - start_time} seconds\")\n"
      ],
      "metadata": {
        "id": "bWXDdkDeUJRG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Experiment Tracking: We'll incorporate tools or libraries for experiment tracking and logging, such as TensorBoard or Weights & Biases. This allows for better monitoring of training progress, visualization of metrics, and comparison of different experiments."
      ],
      "metadata": {
        "id": "oxBNuFdPUuhC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "# Initialize TensorBoard writer\n",
        "writer = SummaryWriter()\n",
        "\n",
        "# Inside the training loop, log metrics\n",
        "writer.add_scalar('Loss/train', total_loss/len(data_loader), epoch)\n",
        "\n",
        "# After training, close the writer\n",
        "writer.close()\n"
      ],
      "metadata": {
        "id": "rZ3t5eqIUJON"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data Augmentation: We'll explore data augmentation techniques to increase the diversity of training data and improve model generalization. This could include random rotations, flips, crops, or color jittering."
      ],
      "metadata": {
        "id": "81C67ovXUxVA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomVerticalFlip(),\n",
        "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
        "    transforms.RandomRotation(degrees=15),\n",
        "    transforms.ToTensor(),\n",
        "])"
      ],
      "metadata": {
        "id": "tBOr2NEPUJKA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Modularization: We'll break down the code into modular functions or classes to improve readability, reusability, and maintainability. For example, separate the data loading, model definition, training loop, and evaluation into distinct functions or classes."
      ],
      "metadata": {
        "id": "mPn7S8n5U3NX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class WeaponDetectionModel(torch.nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super(WeaponDetectionModel, self).__init__()\n",
        "        # Define model architecture\n",
        "\n",
        "    def forward(self, images, targets=None):\n",
        "        # Forward pass\n",
        "\n",
        "    def evaluate(self, data_loader):\n",
        "        # Evaluation logic\n",
        "\n",
        "    def train_step(self, optimizer, loss_function, images, targets):\n",
        "        # Training logic\n",
        "\n",
        "model = WeaponDetectionModel(num_classes)\n"
      ],
      "metadata": {
        "id": "LIv5F4CVUJG3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hyperparameter Tuning: We'll automate hyperparameter tuning using techniques like grid search or random search to find optimal hyperparameters for the model. This can improve model performance and convergence speed."
      ],
      "metadata": {
        "id": "7BXnPW6RVEym"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import make_scorer\n",
        "\n",
        "# Define parameters grid\n",
        "param_grid = {\n",
        "    'lr': [0.001, 0.005, 0.01],\n",
        "    'momentum': [0.9, 0.95, 0.99],\n",
        "    'weight_decay': [0.0001, 0.0005, 0.001]\n",
        "}\n",
        "\n",
        "# Define custom scoring function (e.g., mean average precision)\n",
        "def custom_scorer(model, X, y_true):\n",
        "    y_pred = model.predict(X)\n",
        "    return compute_mean_average_precision(y_pred, y_true)\n",
        "\n",
        "# Initialize GridSearchCV\n",
        "grid_search = GridSearchCV(model, param_grid, scoring=make_scorer(custom_scorer), cv=5)\n"
      ],
      "metadata": {
        "id": "N3nj9vUFU_mw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fine-tuning Pre-trained Model: We'll fine-tune the pre-trained model on our dataset to adapt it to our specific task. This involves freezing certain layers and only updating the parameters of the final layers during training."
      ],
      "metadata": {
        "id": "duERHUuyVII9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example: Fine-tuning by freezing backbone layers\n",
        "for param in model.backbone.parameters():\n",
        "    param.requires_grad = False\n"
      ],
      "metadata": {
        "id": "EgKN1SWNU_j4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Early Stopping: We'll implement early stopping to prevent overfitting and improve generalization. This involves monitoring a validation metric (e.g., validation loss or accuracy) and stopping training when the metric stops improving.\n"
      ],
      "metadata": {
        "id": "zZ87peeEVRn0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "best_loss = float('inf')\n",
        "patience = 3\n",
        "early_stopping_counter = 0\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    # Training loop\n",
        "    # ...\n",
        "\n",
        "    # Validation loop\n",
        "    # Compute validation loss\n",
        "    val_loss = compute_validation_loss(model, val_data_loader)\n",
        "\n",
        "    if val_loss < best_loss:\n",
        "        best_loss = val_loss\n",
        "        early_stopping_counter = 0\n",
        "        # Save the best model\n",
        "        save_model(model, 'best_model.pth')\n",
        "    else:\n",
        "        early_stopping_counter += 1\n",
        "\n",
        "    if early_stopping_counter >= patience:\n",
        "        print(\"Early stopping triggered!\")\n",
        "        break\n"
      ],
      "metadata": {
        "id": "UzLJZWpWU_gv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Learning Rate Scheduler: We'll use a learning rate scheduler to dynamically adjust the learning rate during training, potentially improving convergence and final performance."
      ],
      "metadata": {
        "id": "ELtlr9iwVUCY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.optim.lr_scheduler import StepLR\n",
        "\n",
        "scheduler = StepLR(optimizer, step_size=3, gamma=0.1)\n",
        "# Inside the training loop\n",
        "scheduler.step()\n"
      ],
      "metadata": {
        "id": "qRK3AeiZU_eL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model Ensemble: We can explore the concept of model ensemble, where multiple models are trained independently and their predictions are combined to make a final decision. This can often improve performance compared to using a single model."
      ],
      "metadata": {
        "id": "IznYutH-VfNH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example of simple model averaging ensemble\n",
        "def ensemble_predictions(models, data_loader):\n",
        "    all_predictions = []\n",
        "    for model in models:\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            predictions = []\n",
        "            for images, _ in data_loader:\n",
        "                images = [image.to(device) for image in images]\n",
        "                predictions.extend(model(images))\n",
        "        all_predictions.append(predictions)\n",
        "    return all_predictions\n",
        "\n",
        "def combine_predictions(predictions):\n",
        "    # Combine predictions (e.g., average or majority voting)\n",
        "    # Return final combined predictions\n"
      ],
      "metadata": {
        "id": "1P-b5etUU_bZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data Balancing: If the dataset is imbalanced (e.g., more negative samples than positive samples), we can apply techniques for data balancing such as oversampling, undersampling, or using class weights during training to mitigate the imbalance."
      ],
      "metadata": {
        "id": "8WD7CIHzVi0Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example: Use class weights in the loss function\n",
        "class_weights = compute_class_weights(targets)\n",
        "loss_function = torch.nn.CrossEntropyLoss(weight=class_weights)\n"
      ],
      "metadata": {
        "id": "8Wc_atgtU_YP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model Deployment: Once the model is trained and evaluated, we can deploy it for inference in real-world scenarios. This involves packaging the model into a format suitable for deployment (e.g., ONNX, TorchScript) and integrating it into an application or service."
      ],
      "metadata": {
        "id": "e9-lxQ1ZVmZN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example: Export model to ONNX format\n",
        "dummy_input = torch.randn(1, 3, 224, 224).to(device)\n",
        "torch.onnx.export(model, dummy_input, 'model.onnx', input_names=['input'], output_names=['output'])\n"
      ],
      "metadata": {
        "id": "xwMwNoA7U_Sf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Install necessary libraries\n",
        "#!pip install torch torchvision\n",
        "\n",
        "# Import libraries\n",
        "import torch\n",
        "from torchvision import transforms, datasets\n",
        "from torchvision.models.detection import fasterrcnn_resnet50_fpn\n",
        "from torch.utils.data import DataLoader\n",
        "import torchvision.transforms.functional as F\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "4mWWUa9hJNjZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Define dataset paths\n",
        "image_folder = \"https://github.com/ari-dasci/OD-WeaponDetection/raw/main/Pistol%20detection/Weapons\"\n",
        "annotation_folder = \"https://github.com/ari-dasci/OD-WeaponDetection/raw/main/Pistol%20detection/xmls\"\n",
        "\n",
        "# Define transform\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "# Load dataset\n",
        "dataset = datasets.VOCDetection(root=image_folder, year='2007', image_set='train', download=True, transform=transform)"
      ],
      "metadata": {
        "id": "DV2TBTYYJNf2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Get a few Example of database\n",
        "def show_bbox(image_path):\n",
        "  # convert image path to label path\n",
        "  label_path = image_path.replace('/images/', '/labels/')\n",
        "  label_path = label_path.replace('.jpg', '.txt')\n",
        "  # Open the image and create ImageDraw object for drawing\n",
        "  image = Image.open(image_path)\n",
        "  draw = ImageDraw.Draw(image)\n",
        "  print(image)\n",
        "  with open(label_path, 'r') as f:\n",
        "    for line in f.readlines():\n",
        "      # Split the line into five values\n",
        "      label, x, y, w, h = line.split(' ')\n",
        "\n",
        "      # Convert string into float\n",
        "      x = float(x)\n",
        "      y = float(y)\n",
        "      w = float(w)\n",
        "      h = float(h)\n",
        "\n",
        "      # Convert center position, width, height into\n",
        "      # top-left and bottom-right coordinates\n",
        "      W, H = image.size\n",
        "      x1 = (x - w/2) * W\n",
        "      y1 = (y - h/2) * H\n",
        "      x2 = (x + w/2) * W\n",
        "      y2 = (y + h/2) * H\n",
        "\n",
        "      # Draw the bounding box with red lines\n",
        "      draw.rectangle((x1, y1, x2, y2),outline=(255, 0, 0), width=5)# Line width\n",
        "  return image\n"
      ],
      "metadata": {
        "id": "AqtgWgYRJNcz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import xml.etree.ElementTree as ET\n",
        "from PIL import Image\n",
        "from torchvision import transforms\n",
        "\n",
        "def parse_xml_annotation(xml_file):\n",
        "    tree = ET.parse(xml_file)\n",
        "    root = tree.getroot()\n",
        "\n",
        "    # Extract image path\n",
        "    image_path = root.find('path').text\n",
        "\n",
        "    # Extract object bounding box coordinates and class labels\n",
        "    boxes = []\n",
        "    labels = []\n",
        "    for obj in root.findall('object'):\n",
        "        xmin = int(obj.find('bndbox').find('xmin').text)\n",
        "        ymin = int(obj.find('bndbox').find('ymin').text)\n",
        "        xmax = int(obj.find('bndbox').find('xmax').text)\n",
        "        ymax = int(obj.find('bndbox').find('ymax').text)\n",
        "        label = obj.find('name').text\n",
        "        boxes.append([xmin, ymin, xmax, ymax])\n",
        "        labels.append(label)\n",
        "\n",
        "    return image_path, boxes, labels\n",
        "\n",
        "def my_collate(batch):\n",
        "    images = []\n",
        "    targets = []\n",
        "\n",
        "    for xml_file in batch:\n",
        "        # Parse XML annotation file\n",
        "        image_path, boxes, labels = parse_xml_annotation(xml_file)\n",
        "\n",
        "        # Load image\n",
        "        image = Image.open(image_path).convert(\"RGB\")\n",
        "\n",
        "        # Apply transformations\n",
        "        transform = transforms.Compose([\n",
        "            transforms.Resize((500, 500)),\n",
        "            transforms.ToTensor(),\n",
        "        ])\n",
        "        image = transform(image)\n",
        "\n",
        "        images.append(image)\n",
        "        targets.append({'boxes': boxes, 'labels': labels})\n",
        "\n",
        "    return torch.stack(images), targets\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "z8AfThdPJxj-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage:\n",
        "annotation_folder = \"/content/OD-WeaponDetection/OD-WeaponDetection-master/Pistol detection/xmls\"\n",
        "xml_files = [os.path.join(annotation_folder, file) for file in os.listdir(annotation_folder)]\n",
        "\n",
        "data_loader = DataLoader(xml_files, batch_size=16, shuffle=True, collate_fn=my_collate)\n",
        "\n",
        "\n",
        "\n",
        "# Load pre-trained Faster R-CNN model\n",
        "model = fasterrcnn_resnet50_fpn( weights='DEFAULT')\n",
        "model.to(device)\n",
        "\n",
        "# Define optimizer and loss function\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.03, momentum=0.9)\n",
        "criterion = torch.nn.SmoothL1Loss()"
      ],
      "metadata": {
        "id": "UYJFGN1-J5Zu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "################################# Get a few examples of Photos to present ####################################\n",
        "base_Directory = \"/content/drive/MyDrive/DeepLData/obj_train_data/images/train/\"\n",
        "all_files = os.listdir(base_Directory)\n",
        "image_Paths = []\n",
        "\n",
        "#Get 9 images randomly to see some images (train)\n",
        "for i in range(9):\n",
        "  img = random.choice(all_files)\n",
        "  img_Path = base_Directory + img\n",
        "  image_Paths.append(img_Path)\n",
        "\n",
        "#Create Subplots axes to present data\n",
        "fig, axes = plt.subplots(3, 3, figsize=(10, 10))\n",
        "axes = axes.flatten()\n",
        "\n",
        "#Images to train the data\n",
        "for i, (ax, image_path) in enumerate(zip(axes, image_Paths)):\n",
        "  img = show_bbox(image_path)\n",
        "  ax.imshow(img)"
      ],
      "metadata": {
        "id": "tTUh-AZjJNYi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Load dataset with a subset\n",
        "subset_size = 500  # Change this to the desired subset size\n",
        "subset_dataset = datasets.VOCDetection(root=image_folder, year='2007', image_set='train', download=True, transform=transform)\n",
        "subset_dataset = torch.utils.data.Subset(subset_dataset, range(subset_size))\n",
        "\n",
        "# Define data loader with custom collate function\n",
        "subset_data_loader = DataLoader(subset_dataset, batch_size=16, shuffle=True, num_workers=2, collate_fn=my_collate)"
      ],
      "metadata": {
        "id": "8YSO2jtHJNVO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ... (previous code)\n",
        "\n",
        "# Training loop (light training for demonstration purposes)\n",
        "num_epochs = 50\n",
        "for epoch in range(num_epochs):\n",
        "    for images, targets in data_loader:\n",
        "        images = list(image.to(device) for image in images)\n",
        "\n",
        "        # Modify targets to include necessary keys (assuming bounding box annotations and labels)\n",
        "        targets = [{k: v.to(device) if isinstance(v, torch.Tensor) else v for k, v in t.items()} for t in targets]\n",
        "        for target in targets:\n",
        "            if \"boxes\" not in target:\n",
        "                target[\"boxes\"] = torch.zeros((0, 4), dtype=torch.float32, device=device)\n",
        "            if \"labels\" not in target:\n",
        "                target[\"labels\"] = torch.zeros((0,), dtype=torch.int64, device=device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss_dict = model(images, targets)\n",
        "        losses = sum(loss for loss in loss_dict.values())\n",
        "        losses.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "# ... (continue with the rest of the script)\n"
      ],
      "metadata": {
        "id": "QhpBmevsJNQP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize a sample prediction\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    sample_image, sample_target = dataset[0]\n",
        "    sample_image = sample_image.unsqueeze(0).to(device)\n",
        "    prediction = model([sample_image])[0]\n",
        "\n",
        "# Display the original image and the predicted bounding boxes\n",
        "image = F.to_pil_image(sample_image.cpu().squeeze())\n",
        "draw = ImageDraw.Draw(image)\n",
        "draw.rectangle(prediction['boxes'][0].cpu().numpy(), outline=\"red\")\n",
        "\n",
        "plt.imshow(image)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "dP_7PaTLJNIa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "OZ29NQobT8Aa",
        "outputId": "2f105d9b-058c-44b6-b2e8-d3dbda973ae5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Weapons', 'xmls']\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import requests\n",
        "import zipfile\n",
        "from io import BytesIO\n",
        "\n",
        "# Define the GitHub repository URL\n",
        "repo_url = 'https://github.com/ari-dasci/OD-WeaponDetection/archive/master.zip'\n",
        "\n",
        "# Download the zip file from GitHub\n",
        "response = requests.get(repo_url)\n",
        "zip_file = zipfile.ZipFile(BytesIO(response.content))\n",
        "\n",
        "# Specify the target directory in Colab\n",
        "target_dir = '/content/OD-WeaponDetection'\n",
        "\n",
        "# Extract the contents of the zip file to the target directory\n",
        "zip_file.extractall(target_dir)\n",
        "\n",
        "# Identify the actual directory containing the dataset\n",
        "dataset_dir = os.path.join(target_dir, 'OD-WeaponDetection-master', 'Pistol detection')\n",
        "\n",
        "# Print the contents of the dataset directory\n",
        "print(os.listdir(dataset_dir))\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "tyJgiVMMCRz_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Import necessary libraries\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "# Define root directory\n",
        "root_dir = '/content/OD-WeaponDetection/OD-WeaponDetection-master/Pistol detection'\n",
        "\n",
        "# Define paths to your dataset\n",
        "images_dir = os.path.join(root_dir, 'Weapons')\n",
        "annotations_dir = '/content/OD-WeaponDetection/OD-WeaponDetection-master/Pistol detection/xmls'\n",
        "\n",
        "# Image parameters\n",
        "img_height, img_width = 150, 150\n",
        "batch_size = 32\n",
        "\n",
        "# Create a list of image files\n",
        "image_files = [os.path.join(images_dir, file) for file in os.listdir(images_dir) if file.endswith('.jpg')]\n",
        "\n",
        "# Create a list of annotation files\n",
        "annotation_files = [os.path.join(annotations_dir, file) for file in os.listdir(annotations_dir) if file.endswith('.xml')]\n",
        "\n",
        "# Ensure that corresponding image and annotation files exist\n",
        "image_files = [file for file in image_files if os.path.splitext(file)[0] + '.xml' in annotation_files]\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "train_image_files, test_image_files = train_test_split(image_files, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create data generators with augmentation for training\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True\n",
        ")\n",
        "\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "train_generator = train_datagen.flow_from_list(\n",
        "    train_image_files,\n",
        "    target_size=(img_height, img_width),\n",
        "    batch_size=batch_size,\n",
        "    class_mode='binary'\n",
        ")\n",
        "\n",
        "test_generator = test_datagen.flow_from_list(\n",
        "    test_image_files,\n",
        "    target_size=(img_height, img_width),\n",
        "    batch_size=batch_size,\n",
        "    class_mode='binary'\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "id": "LKqp61a8CQPI",
        "outputId": "fa1c8ebd-7e46-4af3-d1e3-2ffc17fca339"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "With n_samples=0, test_size=0.2 and train_size=None, the resulting train set will be empty. Adjust any of the aforementioned parameters.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-cee15cbc8e56>\u001b[0m in \u001b[0;36m<cell line: 33>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;31m# Split the dataset into training and testing sets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m \u001b[0mtrain_image_files\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_image_files\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_files\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;31m# Create data generators with augmentation for training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_split.py\u001b[0m in \u001b[0;36mtrain_test_split\u001b[0;34m(test_size, train_size, random_state, shuffle, stratify, *arrays)\u001b[0m\n\u001b[1;32m   2560\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2561\u001b[0m     \u001b[0mn_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_num_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2562\u001b[0;31m     n_train, n_test = _validate_shuffle_split(\n\u001b[0m\u001b[1;32m   2563\u001b[0m         \u001b[0mn_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdefault_test_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.25\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2564\u001b[0m     )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_split.py\u001b[0m in \u001b[0;36m_validate_shuffle_split\u001b[0;34m(n_samples, test_size, train_size, default_test_size)\u001b[0m\n\u001b[1;32m   2234\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2235\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mn_train\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2236\u001b[0;31m         raise ValueError(\n\u001b[0m\u001b[1;32m   2237\u001b[0m             \u001b[0;34m\"With n_samples={}, test_size={} and train_size={}, the \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2238\u001b[0m             \u001b[0;34m\"resulting train set will be empty. Adjust any of the \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: With n_samples=0, test_size=0.2 and train_size=None, the resulting train set will be empty. Adjust any of the aforementioned parameters."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "UTnF0Ou8CP4A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Build the CNN model\n",
        "model = models.Sequential()\n",
        "model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(img_height, img_width, 3)))\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "model.add(layers.Conv2D(128, (3, 3), activation='relu'))\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "model.add(layers.Flatten())\n",
        "model.add(layers.Dense(128, activation='relu'))\n",
        "model.add(layers.Dense(1, activation='sigmoid'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(train_generator,\n",
        "                    steps_per_epoch=train_generator.samples // batch_size,\n",
        "                    epochs=10,\n",
        "                    validation_data=test_generator,\n",
        "                    validation_steps=test_generator.samples // batch_size)\n",
        "\n",
        "# Evaluate the model\n",
        "test_loss, test_acc = model.evaluate(test_generator, steps=test_generator.samples // batch_size)\n",
        "print(f'Test Accuracy: {test_acc * 100:.2f}%')\n",
        "\n",
        "# Plot training history\n",
        "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
        "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ONNtsHS5CWx6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ca va ?\n"
      ],
      "metadata": {
        "id": "sVlaWa0rUNez"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "DblpRHrqIjef"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QPtU5R8jKPbU",
        "outputId": "04e56578-7d59-4582-f696-e51a9753af1d"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://host.robots.ox.ac.uk/pascal/VOC/voc2007/VOCtrainval_06-Nov-2007.tar to https://github.com/ari-dasci/OD-WeaponDetection/raw/main/Pistol%20detection/Weapons/VOCtrainval_06-Nov-2007.tar\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 460032000/460032000 [00:13<00:00, 35030423.01it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting https://github.com/ari-dasci/OD-WeaponDetection/raw/main/Pistol%20detection/Weapons/VOCtrainval_06-Nov-2007.tar to https://github.com/ari-dasci/OD-WeaponDetection/raw/main/Pistol%20detection/Weapons\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YvFo5nv_LxSb"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 245
        },
        "id": "-ZLLeXBYSYSO",
        "outputId": "9a4e60ea-1e9a-4ce4-e74e-c0a6f4c8d65b"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/drive/MyDrive/DeepLData/obj_train_data/images/train/'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-c7a7b86f89aa>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m################################# Get a few examples of Photos to present ####################################\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mbase_Directory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"/content/drive/MyDrive/DeepLData/obj_train_data/images/train/\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mall_files\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_Directory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mimage_Paths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/MyDrive/DeepLData/obj_train_data/images/train/'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QgLmG3YxSj5Z"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Define custom collate function\n",
        "def my_collate(batch):\n",
        "    images = [item[0] for item in batch]\n",
        "    targets = [item[1] for item in batch]\n",
        "\n",
        "    # Resize images to a common size (e.g., 500x500)\n",
        "    images = [F.resize(img, (500, 500)) for img in images]\n",
        "\n",
        "    return torch.stack(images), targets\n",
        "\n",
        "# Define data loader with custom collate function\n",
        "data_loader = DataLoader(dataset, batch_size=2, shuffle=True, num_workers=2, collate_fn=my_collate)\n",
        "\n",
        "\n",
        "\n",
        "# Load pre-trained Faster R-CNN model\n",
        "model = fasterrcnn_resnet50_fpn(weights=FasterRCNN_ResNet50_FPN_Weights.DEFAULT)\n",
        "model.to(device)\n",
        "\n",
        "# Define optimizer and loss function\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.03, momentum=0.9)\n",
        "criterion = torch.nn.SmoothL1Loss()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "Id_zF-iNLf8R",
        "outputId": "64dd69c7-1ef4-4944-ebf9-78086ac76f22"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'FasterRCNN_ResNet50_FPN_Weights' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-ed03303267e4>\u001b[0m in \u001b[0;36m<cell line: 17>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m# Load pre-trained Faster R-CNN model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfasterrcnn_resnet50_fpn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFasterRCNN_ResNet50_FPN_Weights\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDEFAULT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'FasterRCNN_ResNet50_FPN_Weights' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "subset_data_loader\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gtBzBkN8Mvrs",
        "outputId": "088d817a-64d9-4038-f21b-a087e2cd87d9"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch.utils.data.dataloader.DataLoader at 0x787b43ad2800>"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8XJ53rzGNRZv",
        "outputId": "794deaa6-a79e-4abb-c13a-1e95e4e078b6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using downloaded and verified file: https://github.com/ari-dasci/OD-WeaponDetection/raw/main/Pistol%20detection/Weapons/VOCtrainval_06-Nov-2007.tar\n",
            "Extracting https://github.com/ari-dasci/OD-WeaponDetection/raw/main/Pistol%20detection/Weapons/VOCtrainval_06-Nov-2007.tar to https://github.com/ari-dasci/OD-WeaponDetection/raw/main/Pistol%20detection/Weapons\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "id": "AazBo24DKSot",
        "outputId": "3668dbec-e67b-4aa3-a895-67c7cea27f30"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'C:\\\\Users\\\\Rob\\\\Desktop\\\\Definitiva\\\\armas (2575).jpg'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-cb5bea6ca932>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mnum_epochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m         \u001b[0mimages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mimage\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    629\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    630\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 631\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    632\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    633\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    673\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    674\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 675\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    676\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    677\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-16-8b1c1f8db139>\u001b[0m in \u001b[0;36mmy_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0;31m# Load image\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m         \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"RGB\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0;31m# Apply transformations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/PIL/Image.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(fp, mode, formats)\u001b[0m\n\u001b[1;32m   3225\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3226\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3227\u001b[0;31m         \u001b[0mfp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuiltins\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3228\u001b[0m         \u001b[0mexclusive_fp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3229\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'C:\\\\Users\\\\Rob\\\\Desktop\\\\Definitiva\\\\armas (2575).jpg'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Training loop (light training for demonstration purposes)\n",
        "num_epochs = 2\n",
        "for epoch in range(num_epochs):\n",
        "    for images, targets in subset_data_loader:\n",
        "        images = list(image.to(device) for image in images)\n",
        "\n",
        "        # Modify targets to include necessary keys (assuming bounding box annotations and labels)\n",
        "        targets = [{k: v.to(device) if isinstance(v, torch.Tensor) else v for k, v in t.items()} for t in targets]\n",
        "        for target in targets:\n",
        "            if \"boxes\" not in target:\n",
        "                target[\"boxes\"] = torch.zeros((0, 4), dtype=torch.float32, device=device)\n",
        "            if \"labels\" not in target:\n",
        "                target[\"labels\"] = torch.zeros((0,), dtype=torch.int64, device=device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss_dict = model(images, targets)\n",
        "        losses = sum(loss for loss in loss_dict.values())\n",
        "        losses.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "# ... (continue with the rest of the script"
      ],
      "metadata": {
        "id": "lHmiKJmtNdgu",
        "outputId": "f01adab0-8c54-427e-e131-58f1abe1b7a9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 634
        }
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "Caught TypeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/worker.py\", line 308, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\", line 54, in fetch\n    return self.collate_fn(data)\n  File \"<ipython-input-21-8b1c1f8db139>\", line 33, in my_collate\n    image_path, boxes, labels = parse_xml_annotation(xml_file)\n  File \"<ipython-input-21-8b1c1f8db139>\", line 7, in parse_xml_annotation\n    tree = ET.parse(xml_file)\n  File \"/usr/lib/python3.10/xml/etree/ElementTree.py\", line 1222, in parse\n    tree.parse(source, parser)\n  File \"/usr/lib/python3.10/xml/etree/ElementTree.py\", line 569, in parse\n    source = open(source, \"rb\")\nTypeError: expected str, bytes or os.PathLike object, not tuple\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-24-fd4f028b890f>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mnum_epochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msubset_data_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m         \u001b[0mimages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mimage\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    629\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    630\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 631\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    632\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    633\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1344\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1345\u001b[0m                 \u001b[0;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_task_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1346\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1347\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1348\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_try_put_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_process_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1370\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_put_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1371\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExceptionWrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1372\u001b[0;31m             \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1373\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1374\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_utils.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0;31m# instantiate since we don't know how to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mexception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    724\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: Caught TypeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/worker.py\", line 308, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\", line 54, in fetch\n    return self.collate_fn(data)\n  File \"<ipython-input-21-8b1c1f8db139>\", line 33, in my_collate\n    image_path, boxes, labels = parse_xml_annotation(xml_file)\n  File \"<ipython-input-21-8b1c1f8db139>\", line 7, in parse_xml_annotation\n    tree = ET.parse(xml_file)\n  File \"/usr/lib/python3.10/xml/etree/ElementTree.py\", line 1222, in parse\n    tree.parse(source, parser)\n  File \"/usr/lib/python3.10/xml/etree/ElementTree.py\", line 569, in parse\n    source = open(source, \"rb\")\nTypeError: expected str, bytes or os.PathLike object, not tuple\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MW-4OurnKUIz",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "outputId": "f3a45549-820f-462d-f1e1-9bcbc13a08bc"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "images is expected to be a list of 3d tensors of shape [C, H, W], got torch.Size([1, 3, 333, 500])",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-25-7c621a051172>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0msample_image\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_target\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0msample_image\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample_image\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mprediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msample_image\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# Display the original image and the predicted bounding boxes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1511\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1518\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1519\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1522\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/models/detection/generalized_rcnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, images, targets)\u001b[0m\n\u001b[1;32m     81\u001b[0m             \u001b[0moriginal_image_sizes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m         \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0;31m# Check for degenerate boxes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1511\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1518\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1519\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1522\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/models/detection/transform.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, images, targets)\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 140\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"images is expected to be a list of 3d tensors of shape [C, H, W], got {image.shape}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    141\u001b[0m             \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m             \u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: images is expected to be a list of 3d tensors of shape [C, H, W], got torch.Size([1, 3, 333, 500])"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ... (previous code)\n",
        "\n",
        "# Evaluation loop\n",
        "model.eval()\n",
        "all_predictions = []\n",
        "all_targets = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for images, targets in data_loader:\n",
        "        images = list(image.to(device) for image in images)\n",
        "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
        "\n",
        "        prediction = model(images)\n",
        "        all_predictions.extend(prediction)\n",
        "        all_targets.extend(targets)\n",
        "\n",
        "# Calculate Average Precision\n",
        "from sklearn.metrics import average_precision_score\n",
        "\n",
        "y_true = []\n",
        "y_scores = []\n",
        "\n",
        "for target, prediction in zip(all_targets, all_predictions):\n",
        "    # Convert box coordinates to numpy arrays for easier handling\n",
        "    target_boxes = target['boxes'].cpu().numpy()\n",
        "    prediction_boxes = prediction['boxes'].cpu().numpy()\n",
        "\n",
        "    # Calculate IoU (Intersection over Union) between target and prediction boxes\n",
        "    iou = calculate_iou(target_boxes, prediction_boxes)\n",
        "\n",
        "    # Determine true positives (IoU > threshold)\n",
        "    true_positives = (iou > 0.5).sum()\n",
        "\n",
        "    # Append ground truth and prediction information\n",
        "    y_true.append(torch.ones(true_positives))\n",
        "    y_true.append(torch.zeros(len(target_boxes) - true_positives))\n",
        "\n",
        "    y_scores.append(prediction['scores'])\n",
        "    y_scores.append(torch.zeros(len(target_boxes) - true_positives))\n",
        "\n",
        "y_true = torch.cat(y_true)\n",
        "y_scores = torch.cat(y_scores)\n",
        "\n",
        "# Calculate Average Precision using sklearn\n",
        "average_precision = average_precision_score(y_true.numpy(), y_scores.numpy())\n",
        "print(f'Average Precision: {average_precision}')\n",
        "\n",
        "# Visualize predictions on a few test images\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    for i in range(3):  # Visualize predictions for the first 3 images\n",
        "        sample_image, sample_target = dataset[i]\n",
        "        sample_image = sample_image.unsqueeze(0).to(device)\n",
        "        prediction = model([sample_image])[0]\n",
        "\n",
        "        # Display the original image and the predicted bounding boxes\n",
        "        image = F.to_pil_image(sample_image.cpu().squeeze())\n",
        "        draw = ImageDraw.Draw(image)\n",
        "\n",
        "        for box, score in zip(prediction['boxes'], prediction['scores']):\n",
        "            if score > 0.5:  # Confidence threshold\n",
        "                draw.rectangle(box.cpu().numpy(), outline=\"red\")\n",
        "\n",
        "        plt.imshow(image)\n",
        "        plt.show()\n"
      ],
      "metadata": {
        "id": "LiX1SAL9K-ZX",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "outputId": "81afec51-d4ab-4ab5-e7f6-a0e93cd2199b"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'C:\\\\Users\\\\Rob\\\\Desktop\\\\Definitiva\\\\armas (1006).jpg'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-26-50bbed28afd9>\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0mimages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mimage\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    629\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    630\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 631\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    632\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    633\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    673\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    674\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 675\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    676\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    677\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-21-8b1c1f8db139>\u001b[0m in \u001b[0;36mmy_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0;31m# Load image\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m         \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"RGB\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0;31m# Apply transformations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/PIL/Image.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(fp, mode, formats)\u001b[0m\n\u001b[1;32m   3225\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3226\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3227\u001b[0;31m         \u001b[0mfp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuiltins\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3228\u001b[0m         \u001b[0mexclusive_fp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3229\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'C:\\\\Users\\\\Rob\\\\Desktop\\\\Definitiva\\\\armas (1006).jpg'"
          ]
        }
      ]
    }
  ]
}